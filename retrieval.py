import json
import os
import pickle
import time
from contextlib import contextmanager
from typing import List, NoReturn, Optional, Tuple, Union

import faiss
import numpy as np
import pandas as pd
from datasets import Dataset, concatenate_datasets, load_from_disk
from sklearn.feature_extraction.text import TfidfVectorizer
from tqdm.auto import tqdm

from transformers import AutoTokenizer
import argparse
import random
import torch
from omegaconf import OmegaConf
from model.Retrieval.BertEncoder import BertEncoder
from data_loader.data_loaders import DenseRetrievalValidDataset, DenseRetrievalDataset

os.environ["TOKENIZERS_PARALLELISM"] = "false"


@contextmanager
def timer(name):
    t0 = time.time()
    yield
    print(f"[{name}] done in {time.time() - t0:.3f} s")


class DenseRetrieval:
    def __init__(
        self,
        config,
        tokenizer,
        encoders,
        data_path: Optional[str] = "./data/wikipedia_documents.json",
    ):
        self.config = config
        self.tokenizer = tokenizer
        self.p_encoder = encoders[0]
        self.q_encoder = encoders[1]
        self.passage_embedding_vectors = None  # get_dense_passage_embedding()ì—ì„œ ìƒì„±

        self.data_path = data_path
        with open(data_path, "r") as f:
            wiki = json.load(f)
        self.contexts = list(dict.fromkeys(w["text"] for w in wiki.values()))
        self.ids = list(range(len(self.contexts)))

    def get_dense_passage_embedding(self):

        if os.path.isfile("./saved_models/retrieval/passage_embedding_vectors.bin"):
            with open("./saved_models/retrieval/passage_embedding_vectors.bin", "rb") as f:
                self.passage_embedding_vectors = pickle.load(f)
            print("ğŸ”¥ Embedding pickle loaded")
        else:
            self.passage_embedding_vectors = []
            p_seqs = self.tokenizer(
                self.contexts,
                padding="max_length",
                max_length=self.config.retrieval.tokenizer.max_context_length,
                truncation=True,
                return_tensors="pt",
            )
            # Dataset
            p_dataset = DenseRetrievalValidDataset(self.data_path, self.config.retrieval.tokenizer.max_context_length, self.tokenizer)
            # DataLoader
            p_dataloader = torch.utils.data.DataLoader(p_dataset, batch_size=self.config.retrieval.train.batch_size, shuffle=False, num_workers=4)
            # Make passage embedding
            for item in tqdm(p_dataloader):
                self.passage_embedding_vectors.extend(
                    self.p_encoder(
                        input_ids=item[0],
                        attention_mask=item[1],
                        token_type_ids=item[2],
                    )
                    .detach()
                    .numpy()
                )
                torch.cuda.empty_cache()
                del item
            self.passage_embedding_vectors = torch.Tensor(self.passage_embedding_vectors).squeeze()  # (56737, 768)

            with open("./saved_models/retrieval/passage_embedding_vectors.bin", "wb") as f:
                pickle.dump(self.passage_embedding_vectors, f)
            print("ğŸ”¥ Embedding pickle saved")

    def retrieve(self, query_or_dataset: Union[str, Dataset], top_k: int = 10):
        assert self.passage_embedding_vectors is not None, "Passage embedding vectors is None. Please run get_dense_passage_embedding() first."
        print("âœ… retrieve start")
        if isinstance(query_or_dataset, str):
            doc_scores, doc_ids = self.get_relevant_doc(query_or_dataset, top_k)

            for i in range(len(doc_scores)):
                print(f"top {i + 1} : {doc_scores[i]:.4f}")
                print(self.contexts[doc_ids[i]])

            return (doc_scores, [self.context[doc_ids[i]] for i in range(len(doc_ids))])

        elif isinstance(query_or_dataset, Dataset):
            correct_cnt = 0
            is_val = False
            total = []
            with timer("query exhaustive search"):
                doc_scores, doc_ids = self.get_relevant_doc_bulk(query_or_dataset["question"], top_k)
            for idx, example in enumerate(tqdm(query_or_dataset, desc="Dense retrieval: ")):
                tmp = {
                    # Queryì™€ í•´ë‹¹ idë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
                    "question": example["question"],
                    "id": example["id"],
                    # Retrieveí•œ Passageì˜ id, contextë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
                    "context_id": doc_ids[idx],
                    "context": " ".join([self.contexts[pid] for pid in doc_ids[idx]]),
                }
                if "context" in example.keys() and "answers" in example.keys():
                    is_val = True
                    # validation ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë©´ ground_truth contextì™€ answerë„ ë°˜í™˜í•©ë‹ˆë‹¤.
                    tmp["original_context"] = example["context"]
                    tmp["answers"] = example["answers"]

                    if tmp["original_context"] in tmp["context"]:
                        correct_cnt += 1
                total.append(tmp)
            cqas = pd.DataFrame(total)
            if is_val == True:
                print(f"Validation Accuracy: {correct_cnt / len(query_or_dataset) * 100:.2f}%")
            return cqas

    def get_relevant_doc(self, query, top_k):
        q_seqs = self.tokenizer(
            [query], max_length=self.config.retrieval.tokenizer.max_question_length, padding="max_length", truncation=True, return_tensors="pt"
        ).to("cuda")
        q_emb = self.q_encoder(**q_seqs)  # (1, 768)

        sim_score = torch.matmul(q_emb, torch.transpose(self.passage_embedding_vectors, 0, 1))  # (1, 56737)

        rank = torch.argsort(sim_score, dim=1, descending=True)
        print("ğŸ˜¡", rank.shape)
        doc_ids = rank[0][:top_k].tolist()
        doc_scores = sim_score[0][doc_ids].tolist()

        return doc_ids, doc_scores

    def get_relevant_doc_bulk(self, queries, top_k):
        print("âœ… get_relevant_doc_bulk start")
        q_seqs = self.tokenizer(
            queries, max_length=self.config.retrieval.tokenizer.max_question_length, padding="max_length", truncation=True, return_tensors="pt"
        )
        print("âœ… q_seqs done")
        q_dataset = DenseRetrievalDataset(
            input_ids=q_seqs["input_ids"], attention_mask=q_seqs["attention_mask"], token_type_ids=q_seqs["token_type_ids"]
        )
        print("âœ… q_dataset done")
        q_dataloader = torch.utils.data.DataLoader(q_dataset, batch_size=1)

        doc_ids = []
        doc_scores = []
        for item in tqdm(q_dataloader):
            q_embs = self.q_encoder(input_ids=item[0], attention_mask=item[1], token_type_ids=item[2])
            for q_emb in q_embs:
                sim_scores = torch.matmul(q_emb, torch.transpose(self.passage_embedding_vectors, 0, 1))
                rank = torch.argsort(sim_scores, dim=0, descending=True)
                doc_ids.append(rank[:top_k].tolist())
                doc_scores.append(sim_scores[rank[:top_k]].tolist())
        return doc_scores, doc_ids


class SparseRetrieval:
    def __init__(
        self,
        tokenize_fn,
        data_path: Optional[str] = "./data/",
        context_path: Optional[str] = "wikipedia_documents.json",
    ):

        """
        Arguments:
            tokenize_fn:
                ê¸°ë³¸ textë¥¼ tokenizeí•´ì£¼ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
                ì•„ë˜ì™€ ê°™ì€ í•¨ìˆ˜ë“¤ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                - lambda x: x.split(' ')
                - Huggingface Tokenizer
                - konlpy.tagì˜ Mecab

            data_path:
                ë°ì´í„°ê°€ ë³´ê´€ë˜ì–´ ìˆëŠ” ê²½ë¡œì…ë‹ˆë‹¤.

            context_path:
                Passageë“¤ì´ ë¬¶ì—¬ìˆëŠ” íŒŒì¼ëª…ì…ë‹ˆë‹¤.

            data_path/context_pathê°€ ì¡´ì¬í•´ì•¼í•©ë‹ˆë‹¤.

        Summary:
            Passage íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ê³  TfidfVectorizerë¥¼ ì„ ì–¸í•˜ëŠ” ê¸°ëŠ¥ì„ í•©ë‹ˆë‹¤.
        """

        self.data_path = data_path
        with open(os.path.join(data_path, context_path), "r", encoding="utf-8") as f:
            wiki = json.load(f)

        self.contexts = list(dict.fromkeys([v["text"] for v in wiki.values()]))  # set ì€ ë§¤ë²ˆ ìˆœì„œê°€ ë°”ë€Œë¯€ë¡œ
        print(f"Lengths of unique contexts : {len(self.contexts)}")
        self.ids = list(range(len(self.contexts)))

        # Transform by vectorizer
        self.tfidfv = TfidfVectorizer(
            tokenizer=tokenize_fn,
            ngram_range=(1, 2),
            max_features=50000,
        )

        self.p_embedding = None  # get_sparse_embedding()ë¡œ ìƒì„±í•©ë‹ˆë‹¤
        self.indexer = None  # build_faiss()ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    def get_sparse_embedding(self):

        """
        Summary:
            Passage Embeddingì„ ë§Œë“¤ê³ 
            TFIDFì™€ Embeddingì„ pickleë¡œ ì €ì¥í•©ë‹ˆë‹¤.
            ë§Œì•½ ë¯¸ë¦¬ ì €ì¥ëœ íŒŒì¼ì´ ìˆìœ¼ë©´ ì €ì¥ëœ pickleì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
        """

        # Pickleì„ ì €ì¥í•©ë‹ˆë‹¤.
        pickle_name = f"sparse_embedding.bin"
        tfidfv_name = f"tfidv.bin"
        emd_path = os.path.join(self.data_path, pickle_name)
        tfidfv_path = os.path.join(self.data_path, tfidfv_name)

        if os.path.isfile(emd_path) and os.path.isfile(tfidfv_path):
            with open(emd_path, "rb") as file:
                self.p_embedding = pickle.load(file)
            with open(tfidfv_path, "rb") as file:
                self.tfidfv = pickle.load(file)
            print("Embedding pickle load.")
        else:
            print("Build passage embedding")
            self.p_embedding = self.tfidfv.fit_transform(self.contexts)
            print(self.p_embedding.shape)
            with open(emd_path, "wb") as file:
                pickle.dump(self.p_embedding, file)
            with open(tfidfv_path, "wb") as file:
                pickle.dump(self.tfidfv, file)
            print("Embedding pickle saved.")

    def build_faiss(self, num_clusters=64) -> NoReturn:

        """
        Summary:
            ì†ì„±ìœ¼ë¡œ ì €ì¥ë˜ì–´ ìˆëŠ” Passage Embeddingì„
            Faiss indexerì— fitting ì‹œì¼œë†“ìŠµë‹ˆë‹¤.
            ì´ë ‡ê²Œ ì €ì¥ëœ indexerëŠ” `get_relevant_doc`ì—ì„œ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

        Note:
            FaissëŠ” Buildí•˜ëŠ”ë° ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê¸° ë•Œë¬¸ì—,
            ë§¤ë²ˆ ìƒˆë¡­ê²Œ buildí•˜ëŠ” ê²ƒì€ ë¹„íš¨ìœ¨ì ì…ë‹ˆë‹¤.
            ê·¸ë ‡ê¸° ë•Œë¬¸ì— buildëœ index íŒŒì¼ì„ ì €ì •í•˜ê³  ë‹¤ìŒì— ì‚¬ìš©í•  ë•Œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
            ë‹¤ë§Œ ì´ index íŒŒì¼ì€ ìš©ëŸ‰ì´ 1.4Gb+ ì´ê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ num_clustersë¡œ ì‹œí—˜í•´ë³´ê³ 
            ì œì¼ ì ì ˆí•œ ê²ƒì„ ì œì™¸í•˜ê³  ëª¨ë‘ ì‚­ì œí•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
        """

        indexer_name = f"faiss_clusters{num_clusters}.index"
        indexer_path = os.path.join(self.data_path, indexer_name)
        if os.path.isfile(indexer_path):
            print("Load Saved Faiss Indexer.")
            self.indexer = faiss.read_index(indexer_path)

        else:
            p_emb = self.p_embedding.astype(np.float32).toarray()
            emb_dim = p_emb.shape[-1]

            num_clusters = num_clusters
            quantizer = faiss.IndexFlatL2(emb_dim)

            self.indexer = faiss.IndexIVFScalarQuantizer(quantizer, quantizer.d, num_clusters, faiss.METRIC_L2)
            self.indexer.train(p_emb)
            self.indexer.add(p_emb)
            faiss.write_index(self.indexer, indexer_path)
            print("Faiss Indexer Saved.")

    def retrieve(self, query_or_dataset: Union[str, Dataset], topk: Optional[int] = 1) -> Union[Tuple[List, List], pd.DataFrame]:

        """
        Arguments:
            query_or_dataset (Union[str, Dataset]):
                strì´ë‚˜ Datasetìœ¼ë¡œ ì´ë£¨ì–´ì§„ Queryë¥¼ ë°›ìŠµë‹ˆë‹¤.
                str í˜•íƒœì¸ í•˜ë‚˜ì˜ queryë§Œ ë°›ìœ¼ë©´ `get_relevant_doc`ì„ í†µí•´ ìœ ì‚¬ë„ë¥¼ êµ¬í•©ë‹ˆë‹¤.
                Dataset í˜•íƒœëŠ” queryë¥¼ í¬í•¨í•œ HF.Datasetì„ ë°›ìŠµë‹ˆë‹¤.
                ì´ ê²½ìš° `get_relevant_doc_bulk`ë¥¼ í†µí•´ ìœ ì‚¬ë„ë¥¼ êµ¬í•©ë‹ˆë‹¤.
            topk (Optional[int], optional): Defaults to 1.
                ìƒìœ„ ëª‡ ê°œì˜ passageë¥¼ ì‚¬ìš©í•  ê²ƒì¸ì§€ ì§€ì •í•©ë‹ˆë‹¤.

        Returns:
            1ê°œì˜ Queryë¥¼ ë°›ëŠ” ê²½ìš°  -> Tuple(List, List)
            ë‹¤ìˆ˜ì˜ Queryë¥¼ ë°›ëŠ” ê²½ìš° -> pd.DataFrame: [description]

        Note:
            ë‹¤ìˆ˜ì˜ Queryë¥¼ ë°›ëŠ” ê²½ìš°,
                Ground Truthê°€ ìˆëŠ” Query (train/valid) -> ê¸°ì¡´ Ground Truth Passageë¥¼ ê°™ì´ ë°˜í™˜í•©ë‹ˆë‹¤.
                Ground Truthê°€ ì—†ëŠ” Query (test) -> Retrievalí•œ Passageë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
        """

        assert self.p_embedding is not None, "get_sparse_embedding() ë©”ì†Œë“œë¥¼ ë¨¼ì € ìˆ˜í–‰í•´ì¤˜ì•¼í•©ë‹ˆë‹¤."

        if isinstance(query_or_dataset, str):
            doc_scores, doc_indices = self.get_relevant_doc(query_or_dataset, k=topk)
            print("[Search query]\n", query_or_dataset, "\n")

            for i in range(topk):
                print(f"Top-{i+1} passage with score {doc_scores[i]:4f}")
                print(self.contexts[doc_indices[i]])

            return (doc_scores, [self.contexts[doc_indices[i]] for i in range(topk)])

        elif isinstance(query_or_dataset, Dataset):

            # Retrieveí•œ Passageë¥¼ pd.DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
            total = []
            with timer("query exhaustive search"):
                doc_scores, doc_indices = self.get_relevant_doc_bulk(query_or_dataset["question"], k=topk)
            for idx, example in enumerate(tqdm(query_or_dataset, desc="Sparse retrieval: ")):
                tmp = {
                    # Queryì™€ í•´ë‹¹ idë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
                    "question": example["question"],
                    "id": example["id"],
                    # Retrieveí•œ Passageì˜ id, contextë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
                    "context": " ".join([self.contexts[pid] for pid in doc_indices[idx]]),
                }
                if "context" in example.keys() and "answers" in example.keys():
                    # validation ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë©´ ground_truth contextì™€ answerë„ ë°˜í™˜í•©ë‹ˆë‹¤.
                    tmp["original_context"] = example["context"]
                    tmp["answers"] = example["answers"]
                total.append(tmp)

            cqas = pd.DataFrame(total)
            return cqas

    def get_relevant_doc(self, query: str, k: Optional[int] = 1) -> Tuple[List, List]:

        """
        Arguments:
            query (str):
                í•˜ë‚˜ì˜ Queryë¥¼ ë°›ìŠµë‹ˆë‹¤.
            k (Optional[int]): 1
                ìƒìœ„ ëª‡ ê°œì˜ Passageë¥¼ ë°˜í™˜í• ì§€ ì •í•©ë‹ˆë‹¤.
        Note:
            vocab ì— ì—†ëŠ” ì´ìƒí•œ ë‹¨ì–´ë¡œ query í•˜ëŠ” ê²½ìš° assertion ë°œìƒ (ì˜ˆ) ë™£ë™‡?
        """

        with timer("transform"):
            query_vec = self.tfidfv.transform([query])
        assert np.sum(query_vec) != 0, "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì´ ì˜¤ë¥˜ëŠ” ë³´í†µ queryì— vectorizerì˜ vocabì— ì—†ëŠ” ë‹¨ì–´ë§Œ ì¡´ì¬í•˜ëŠ” ê²½ìš° ë°œìƒí•©ë‹ˆë‹¤."

        with timer("query ex search"):
            result = query_vec * self.p_embedding.T
        if not isinstance(result, np.ndarray):
            result = result.toarray()

        sorted_result = np.argsort(result.squeeze())[::-1]
        doc_score = result.squeeze()[sorted_result].tolist()[:k]
        doc_indices = sorted_result.tolist()[:k]
        return doc_score, doc_indices

    def get_relevant_doc_bulk(self, queries: List, k: Optional[int] = 1) -> Tuple[List, List]:

        """
        Arguments:
            queries (List):
                í•˜ë‚˜ì˜ Queryë¥¼ ë°›ìŠµë‹ˆë‹¤.
            k (Optional[int]): 1
                ìƒìœ„ ëª‡ ê°œì˜ Passageë¥¼ ë°˜í™˜í• ì§€ ì •í•©ë‹ˆë‹¤.
        Note:
            vocab ì— ì—†ëŠ” ì´ìƒí•œ ë‹¨ì–´ë¡œ query í•˜ëŠ” ê²½ìš° assertion ë°œìƒ (ì˜ˆ) ë™£ë™‡?
        """

        query_vec = self.tfidfv.transform(queries)
        assert np.sum(query_vec) != 0, "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì´ ì˜¤ë¥˜ëŠ” ë³´í†µ queryì— vectorizerì˜ vocabì— ì—†ëŠ” ë‹¨ì–´ë§Œ ì¡´ì¬í•˜ëŠ” ê²½ìš° ë°œìƒí•©ë‹ˆë‹¤."

        result = query_vec * self.p_embedding.T
        if not isinstance(result, np.ndarray):
            result = result.toarray()
        doc_scores = []
        doc_indices = []
        for i in range(result.shape[0]):
            sorted_result = np.argsort(result[i, :])[::-1]
            doc_scores.append(result[i, :][sorted_result].tolist()[:k])
            doc_indices.append(sorted_result.tolist()[:k])
        return doc_scores, doc_indices

    def retrieve_faiss(self, query_or_dataset: Union[str, Dataset], topk: Optional[int] = 1) -> Union[Tuple[List, List], pd.DataFrame]:

        """
        Arguments:
            query_or_dataset (Union[str, Dataset]):
                strì´ë‚˜ Datasetìœ¼ë¡œ ì´ë£¨ì–´ì§„ Queryë¥¼ ë°›ìŠµë‹ˆë‹¤.
                str í˜•íƒœì¸ í•˜ë‚˜ì˜ queryë§Œ ë°›ìœ¼ë©´ `get_relevant_doc`ì„ í†µí•´ ìœ ì‚¬ë„ë¥¼ êµ¬í•©ë‹ˆë‹¤.
                Dataset í˜•íƒœëŠ” queryë¥¼ í¬í•¨í•œ HF.Datasetì„ ë°›ìŠµë‹ˆë‹¤.
                ì´ ê²½ìš° `get_relevant_doc_bulk`ë¥¼ í†µí•´ ìœ ì‚¬ë„ë¥¼ êµ¬í•©ë‹ˆë‹¤.
            topk (Optional[int], optional): Defaults to 1.
                ìƒìœ„ ëª‡ ê°œì˜ passageë¥¼ ì‚¬ìš©í•  ê²ƒì¸ì§€ ì§€ì •í•©ë‹ˆë‹¤.

        Returns:
            1ê°œì˜ Queryë¥¼ ë°›ëŠ” ê²½ìš°  -> Tuple(List, List)
            ë‹¤ìˆ˜ì˜ Queryë¥¼ ë°›ëŠ” ê²½ìš° -> pd.DataFrame: [description]

        Note:
            ë‹¤ìˆ˜ì˜ Queryë¥¼ ë°›ëŠ” ê²½ìš°,
                Ground Truthê°€ ìˆëŠ” Query (train/valid) -> ê¸°ì¡´ Ground Truth Passageë¥¼ ê°™ì´ ë°˜í™˜í•©ë‹ˆë‹¤.
                Ground Truthê°€ ì—†ëŠ” Query (test) -> Retrievalí•œ Passageë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
            retrieveì™€ ê°™ì€ ê¸°ëŠ¥ì„ í•˜ì§€ë§Œ faiss.indexerë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """

        assert self.indexer is not None, "build_faiss()ë¥¼ ë¨¼ì € ìˆ˜í–‰í•´ì£¼ì„¸ìš”."

        if isinstance(query_or_dataset, str):
            doc_scores, doc_indices = self.get_relevant_doc_faiss(query_or_dataset, k=topk)
            print("[Search query]\n", query_or_dataset, "\n")

            for i in range(topk):
                print("Top-%d passage with score %.4f" % (i + 1, doc_scores[i]))
                print(self.contexts[doc_indices[i]])

            return (doc_scores, [self.contexts[doc_indices[i]] for i in range(topk)])

        elif isinstance(query_or_dataset, Dataset):

            # Retrieveí•œ Passageë¥¼ pd.DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
            queries = query_or_dataset["question"]
            total = []

            with timer("query faiss search"):
                doc_scores, doc_indices = self.get_relevant_doc_bulk_faiss(queries, k=topk)
            for idx, example in enumerate(tqdm(query_or_dataset, desc="Sparse retrieval: ")):
                tmp = {
                    # Queryì™€ í•´ë‹¹ idë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
                    "question": example["question"],
                    "id": example["id"],
                    # Retrieveí•œ Passageì˜ id, contextë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
                    "context": " ".join([self.contexts[pid] for pid in doc_indices[idx]]),
                }
                if "context" in example.keys() and "answers" in example.keys():
                    # validation ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë©´ ground_truth contextì™€ answerë„ ë°˜í™˜í•©ë‹ˆë‹¤.
                    tmp["original_context"] = example["context"]
                    tmp["answers"] = example["answers"]
                total.append(tmp)

            return pd.DataFrame(total)

    def get_relevant_doc_faiss(self, query: str, k: Optional[int] = 1) -> Tuple[List, List]:

        """
        Arguments:
            query (str):
                í•˜ë‚˜ì˜ Queryë¥¼ ë°›ìŠµë‹ˆë‹¤.
            k (Optional[int]): 1
                ìƒìœ„ ëª‡ ê°œì˜ Passageë¥¼ ë°˜í™˜í• ì§€ ì •í•©ë‹ˆë‹¤.
        Note:
            vocab ì— ì—†ëŠ” ì´ìƒí•œ ë‹¨ì–´ë¡œ query í•˜ëŠ” ê²½ìš° assertion ë°œìƒ (ì˜ˆ) ë™£ë™‡?
        """

        query_vec = self.tfidfv.transform([query])
        assert np.sum(query_vec) != 0, "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì´ ì˜¤ë¥˜ëŠ” ë³´í†µ queryì— vectorizerì˜ vocabì— ì—†ëŠ” ë‹¨ì–´ë§Œ ì¡´ì¬í•˜ëŠ” ê²½ìš° ë°œìƒí•©ë‹ˆë‹¤."

        q_emb = query_vec.toarray().astype(np.float32)
        with timer("query faiss search"):
            D, I = self.indexer.search(q_emb, k)

        return D.tolist()[0], I.tolist()[0]

    def get_relevant_doc_bulk_faiss(self, queries: List, k: Optional[int] = 1) -> Tuple[List, List]:

        """
        Arguments:
            queries (List):
                í•˜ë‚˜ì˜ Queryë¥¼ ë°›ìŠµë‹ˆë‹¤.
            k (Optional[int]): 1
                ìƒìœ„ ëª‡ ê°œì˜ Passageë¥¼ ë°˜í™˜í• ì§€ ì •í•©ë‹ˆë‹¤.
        Note:
            vocab ì— ì—†ëŠ” ì´ìƒí•œ ë‹¨ì–´ë¡œ query í•˜ëŠ” ê²½ìš° assertion ë°œìƒ (ì˜ˆ) ë™£ë™‡?
        """

        query_vecs = self.tfidfv.transform(queries)
        assert np.sum(query_vecs) != 0, "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì´ ì˜¤ë¥˜ëŠ” ë³´í†µ queryì— vectorizerì˜ vocabì— ì—†ëŠ” ë‹¨ì–´ë§Œ ì¡´ì¬í•˜ëŠ” ê²½ìš° ë°œìƒí•©ë‹ˆë‹¤."

        q_embs = query_vecs.toarray().astype(np.float32)
        D, I = self.indexer.search(q_embs, k)

        return D.tolist(), I.tolist()


if __name__ == "__main__":
    """
    í•˜ì´í¼ íŒŒë¼ë¯¸í„° ë“± ê°ì¢… ì„¤ì •ê°’ì„ ì…ë ¥ë°›ìŠµë‹ˆë‹¤
    í„°ë¯¸ë„ ì‹¤í–‰ ì˜ˆì‹œ : python3 run.py --batch_size=64 ...
    ì‹¤í–‰ ì‹œ '--batch_size=64' ê°™ì€ ì¸ìë¥¼ ì…ë ¥í•˜ì§€ ì•Šìœ¼ë©´ default ê°’ì´ ê¸°ë³¸ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤
    """
    parser = argparse.ArgumentParser()

    parser.add_argument("--config", "-c", type=str, default="base_config")
    parser.add_argument(
        "--saved_model",
        "-s",
        default=None,
        help="ì €ì¥ëœ ëª¨ë¸ì˜ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. ì˜ˆì‹œ: saved_models/klue/roberta-small/epoch=?-step=?.ckpt ë˜ëŠ” save_models/model.pt",
    )
    args, _ = parser.parse_known_args()
    config = OmegaConf.load(f"./config/{args.config}.yaml")

    SEED = config.utils.seed
    random.seed(SEED)
    np.random.seed(SEED)
    torch.manual_seed(SEED)
    torch.cuda.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.benchmark = False
    torch.use_deterministic_algorithms(True)

    tokenizer = AutoTokenizer.from_pretrained(config.retrieval.model.name)
    p_encoder = BertEncoder.from_pretrained("./saved_models/retrieval/p_encoder_best_top_10")
    q_encoder = BertEncoder.from_pretrained("./saved_models/retrieval/q_encoder_best_top_10")

    retrieval = DenseRetrieval(config=config, tokenizer=tokenizer, encoders=(p_encoder, q_encoder))
    retrieval.get_dense_passage_embedding()

    del p_encoder

    # datasetì„ ë„£ì–´ì£¼ì–´ì•¼ í•¨
    ## test dataì¼ ê²½ìš°ì™€ train dataì¼ ê²½ìš° ì¶œë ¥ë˜ëŠ”ê²Œ ë‹¤ë¦„
    dataset = load_from_disk("./data/train_dataset/validation")
    df = retrieval.retrieve(dataset, top_k=10)

    df.to_csv("result.csv", index=False, encoding="utf-8-sig")

    # âœ…
    # import argparse

    # parser = argparse.ArgumentParser(description="")
    # parser.add_argument("--dataset_name", metavar="./data/train_dataset", type=str, help="")
    # parser.add_argument(
    #     "--model_name_or_path",
    #     metavar="bert-base-multilingual-cased",
    #     type=str,
    #     help="",
    # )
    # parser.add_argument("--data_path", metavar="./data", type=str, help="")
    # parser.add_argument("--context_path", metavar="wikipedia_documents", type=str, help="")
    # parser.add_argument("--use_faiss", metavar=False, type=bool, help="")

    # args = parser.parse_args()

    # # Test sparse
    # org_dataset = load_from_disk(args.dataset_name)
    # full_ds = concatenate_datasets(
    #     [
    #         org_dataset["train"].flatten_indices(),
    #         org_dataset["validation"].flatten_indices(),
    #     ]
    # )  # train dev ë¥¼ í•©ì¹œ 4192 ê°œ ì§ˆë¬¸ì— ëŒ€í•´ ëª¨ë‘ í…ŒìŠ¤íŠ¸
    # print("*" * 40, "query dataset", "*" * 40)
    # print(full_ds)

    # from transformers import AutoTokenizer

    # tokenizer = AutoTokenizer.from_pretrained(
    #     args.model_name_or_path,
    #     use_fast=False,
    # )

    # retriever = SparseRetrieval(
    #     tokenize_fn=tokenizer.tokenize,
    #     data_path=args.data_path,
    #     context_path=args.context_path,
    # )

    # query = "ëŒ€í†µë ¹ì„ í¬í•¨í•œ ë¯¸êµ­ì˜ í–‰ì •ë¶€ ê²¬ì œê¶Œì„ ê°–ëŠ” êµ­ê°€ ê¸°ê´€ì€?"

    # if args.use_faiss:

    #     # test single query
    #     with timer("single query by faiss"):
    #         scores, indices = retriever.retrieve_faiss(query)

    #     # test bulk
    #     with timer("bulk query by exhaustive search"):
    #         df = retriever.retrieve_faiss(full_ds)
    #         df["correct"] = df["original_context"] == df["context"]

    #         print("correct retrieval result by faiss", df["correct"].sum() / len(df))

    # else:
    #     with timer("bulk query by exhaustive search"):
    #         df = retriever.retrieve(full_ds)
    #         df["correct"] = df["original_context"] == df["context"]
    #         print(
    #             "correct retrieval result by exhaustive search",
    #             df["correct"].sum() / len(df),
    #         )

    #     with timer("single query by exhaustive search"):
    #         scores, indices = retriever.retrieve(query)
