path:
  train: ../data/train_dataset
  predict: ../data/test_dataset
  context: ../data/wikipedia_documents.json

retriever:
  type: sparse # sparse or dense
  topk: 10
  sparse:
    lsa: True # apply LSA or not
    lsa_num_features: 1024 # default: 100.
  faiss:
    num_clusters: 64 # default: 64. The number of clusters

model:
  name: klue/roberta-base

tokenizer:
  max_length: 256
  padding: max_length
  stride: 128
  return_token_type_ids: False # False for Roberta models

optimizer: # default AdamW
  learning_rate: 1e-5
  weight_decay: 0
  adam_beta1: 0.9 # The beta1 hyperparameter for the AdamW optimizer.
  adam_beta2: 0.999 # The beta2 hyperparameter for the AdamW optimizer.
  adam_epsilon: 1e-8 # The epsilon hyperparameter for the AdamW optimizer.
  lr_scheduler_type: linear
  warmup_ratio: 0.5

train:
  output_dir: saved_models/klue/roberta-base/LWJ_12-22-18-40/checkpoint-9500 # default: saved_models/model/{wandb_name}_{time}
  num_train_epochs: 5
  fp16: False
  save_strategy : steps
  save_steps: 500
  save_total_limit: 1 # default 1: save the last and the best
  load_best_model_at_end: False 

utils:
  seed: 42
  num_workers: 1
  overwrite_cache: False
  max_answer_length: 30

wandb:
  team: next-level-potato # team account name
  project: MRC # project name
  name: # 실험자 명
  tags: [] # tag