path:
  train: ../data/train_dataset
  predict: ../data/test_dataset
  data: ../data/
  context: ../data/wikipedia_documents.json
  resume: 

model:
  name_or_path: klue/roberta-base 

tokenizer:
  max_length: 384
  padding: max_length
  stride: 128
  return_token_type_ids: True # False for Roberta models

optimizer: # default AdamW
  learning_rate: 1e-5
  weight_decay: 0
  adam_beta1: 0.9 # The beta1 hyperparameter for the AdamW optimizer.
  adam_beta2: 0.999 # The beta2 hyperparameter for the AdamW optimizer.
  adam_epsilon: 1e-8 # The epsilon hyperparameter for the AdamW optimizer.
  lr_scheduler_type: linear
  warmup_ratio: 0.5

train:
  output_dir: # default: saved_models/model/{wandb_name}_{time}
  num_train_epochs: 5
  fp16: False
  save_strategy : epoch # steps or epoch
  save_steps: 500
  save_total_limit: 1 # default 1: save the last and the best
  load_best_model_at_end: False 

retriever:
  type: sparse # sparse, dense or hybrid
  topk: 10

sparse:
  tfidf_num_features: 50000 # default: 50000. max_num_features for tfidf vectorizer
  lsa: False # apply LSA or not
  lsa_num_features: 1024 # default: 100.

dense: 
  path:
    train: ./data/train_dataset/train
    valid:  ./data/wikipedia_documents.json
    predict: 

  model:
    name_or_path: klue/bert-base
    best_p_encoder_path: "saved_models/DPR/encoder/p_encoder_best_top_30" 
    best_q_encoder_path: "saved_models/DPR/encoder/q_encoder_best_top_30"
    saved_p_embs_epoch: 19 # 사용할 saved p_embs epoch (inference 시 사용)

  tokenizer:
    max_context_length: 512
    max_question_length: 256

  optimizer:
    name: AdamW
    learning_rate: 1e-5
    weight_decay: 0.01
    gradient_accumulation_steps: 1

  train:
    output_dir: # default: saved_models/DPR/model/{wandb_name}_{time}
    num_train_epochs: 30
    batch_size: 4
    hard_negative: True

  utils:
    valid_analysis: False # train epoch마다 validation 결과를 csv로 저장할지 여부

faiss:
  use_faiss: False # default: False
  num_clusters: 64 # default: 64. The number of clusters
  metric: inner_product # default: inner_product . l2 or inner_product available.

utils:
  seed: 42
  overwrite_cache: False
  max_answer_length: 30

wandb:
  team: next-level-potato # team account name
  project: MRC # project name
  name:  # 실험자 명
  tags: [] # tag

hf_hub:
  push_to_hub: False # whether to push to huggingface hub the pretrained model & tokenizer
  save_name: # name to register. e.g. nlpotato/roberta-base-new-model
